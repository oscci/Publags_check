---
title: "ScraperHindawi"
author: "Dorothy Bishop"
date: '2023-01-24'
output: html_document
---

```{r setup, include=FALSE}
#Updated and tidied up for Publags_check project
knitr::opts_chunk$set(echo = TRUE)
require(stringr)
require(rvest)
require(here)
```

itees	163
cmmi	91
scanning	255
jfs	185
oti	239
am	19
abb	35
jchem	172
jcse	176
ijac	119
emi	101
bmi	46
ddns	98
jo	199
ast	4
complexity	85

```{r readarticles}


jfilename<-here('Hjournals.csv')  #
myjournals<-read.csv(jfilename)

jnums <- c(87, 103, 273, 88, 181, 228, 187, 209, 260, 224, 99, 258, 163, 91, 255, 185, 239, 19, 35, 172, 176, 119, 101, 46, 98, 199, 4, 85)
for (thisj in jnums){ #row number for this journal

myjname<-myjournals$name[thisj]
print(myjname)
myshortname<-myjournals$shortname[thisj]
myyear<-2023

artdf<-read.csv(here('H_articles.csv'))
thisrow<-nrow(artdf) #we'll always add to end of df
#-------------------------------------------------------------------------------
#Want to find the maximum N webpages for this journal (and year) - though we will stick with 2022 for now
#-------------------------------------------------------------------------------
#I can't find an easy way to get this, so we will just have to do an incremental loop, and catch errors when the page does not exist.
#In fact, with the test journal, biomed research international, it does not crash when attempting to open a page with no contents - it just has an empty list. So this might work?


#-------------------------------------------------------------------------------
#Articles in journal with article number
#-------------------------------------------------------------------------------

maxpage<-5 #last page of results - we guess at this, it's usually under 200
lastpage<-maxpage
for (thispage in seq(maxpage,1,-1)){  #count down so know when to stop
print(thispage)
 j<-paste0("https://www.hindawi.com/journals/",myshortname,"/contents/year/",myyear,"/page/",thispage) 
 
myart <- read_html(j)
artlist<-myart %>%
  html_nodes("h2") %>%
  html_text()
#artlist

if(length(artlist)<3){lastpage<-lastpage-1}
if(length(artlist)>2){ #blank issues ignored


#The journal TOC does include special issues, but we need to find how they are flagged - it does show up on the page that lists titles
#We then need the article number, 

artn<-myart %>%
  html_nodes("li") %>%
  html_text()
#artn
artIDs<-artn[grepl('- Article ID',artn)]
 artIDs<-gsub("- Article ID ","",artIDs)  #article IDs!
artIDs

#-------------------------------------------------------------------------------
#Open file for each article number and write relevant info to df
#-------------------------------------------------------------------------------
for (a in 1:length(artIDs)){
  thisrow<-thisrow+1
  artdf[thisrow,]<-NA
  artname<-paste0("https://www.hindawi.com/journals/",myshortname,"/",myyear,"/",artIDs[a]) 


  
  mydetails <- read_html(artname)
  # #li gives reference list!  body as node gives whole article
  # mydetail <- mydetails %>%
  #   html_nodes("head") %>%
  #   html_text()
  # mydetail

  mydetail<-mydetails %>%
   html_elements(".articleHeader")%>%       
      html_text()
mydetail
#V close to what we need - only problem is no gap between authors and title. Has dates and academic editor!
#Dates of publication etc - can strip of in reverse
mypub<-NA
  publoc<-unlist(gregexpr('Published', mydetail))
  if (publoc>0){
 mypub<-substr(mydetail, (publoc+9),nchar(mydetail))
  mydetail<-substr(mydetail,1,(publoc-1))
  }
myacc<-NA
  accloc<-unlist(gregexpr('Accepted', mydetail))
  if (accloc>0){
 myacc<-substr(mydetail, (accloc+8),nchar(mydetail))
  mydetail<-substr(mydetail,1,(accloc-1))
  }
  myrev<-NA
  revloc<-unlist(gregexpr('Revised', mydetail))
  if (revloc>0){
 myrev<-substr(mydetail, (revloc+7),nchar(mydetail))
  mydetail<-substr(mydetail,1,(revloc-1))
  }
    myrec<-NA
  recloc<-unlist(gregexpr('Received', mydetail))
  if (recloc>0){
 myrec<-substr(mydetail, (recloc+8),nchar(mydetail))
  mydetail<-substr(mydetail,1,(recloc-1))
  }
  myed<-NA
    edloc<-unlist(gregexpr('Editor', mydetail))
  if (edloc>0){
 myed<-substr(mydetail, (edloc+8),nchar(mydetail))
  mydetail<-substr(mydetail,1,(edloc-1))
  }


#Authors all in one list
  myauthors<-mydetails %>%
   html_elements(".articleHeader__authors")%>%       
      html_text()
myauthors
#Authors separate
myauthor1<-mydetails %>%
   html_elements(".articleHeader__authors_author")%>%       
      html_text()
myauthor1


  mytitle<-mydetails %>%
   html_elements(".articleHeader__title")%>%       
      html_text()
mytitle

mysi<-0
 mys<-mydetails %>%
   html_elements(".articleHeader__specialIssue_title")%>%       
   html_text()
   if(length(mys)>0){
     mysi<-mys}

 mydetail<-mydetails %>%
   html_elements(".articleHeader__meta")%>%       
  html_text()
  mydetail
  posdoi<-unlist(gregexpr('doi', mydetail))
 mydoi<-substr(mydetail, (posdoi+8),nchar(mydetail))

#NB emails are in there ; bit harder to extract!


  hrefs<-mydetails %>%
#   html_nodes(".articleHeader__authors__author")%>%  
   html_elements("a")%>% 
     html_attr("href")
  # hrefs  #YESSS!!
   
   myemail<-hrefs[grepl('mailto:',hrefs)][1] #first mention only
    myemail<-gsub("mailto:","",myemail)  #strip out mailto text
 

artdf$jnumber[thisrow]<-thisj
artdf$Journal[thisrow]<-myjname
artdf$year[thisrow]<-myyear
artdf$doi[thisrow]<-mydoi
artdf$title[thisrow]<-mytitle
artdf$authors[thisrow]<-myauthors
artdf$received[thisrow]<-myrec
artdf$revised[thisrow]<-myrev
artdf$accepted[thisrow]<-myacc
artdf$published[thisrow]<-mypub
artdf$editor[thisrow]<-myed
artdf$ID[thisrow]<-artIDs[a]
artdf$SI[thisrow]<-mysi
artdf$row[thisrow]<-thisrow
artdf$affiliation[thisrow]<-myemail
}

}
}
nuname<-paste0('Hindawi_results/H_articles_',myyear,'_',myshortname,'_',lastpage,'p.csv' )
write.csv(artdf,here(nuname),row.names=F)
}
```


```{r readspecissues}
jlist<-read.csv('smut_journals.csv')$shortname[1:10]

#create empty data frame to hold details
sidf<-data.frame(matrix(NA,nrow=0,ncol=13))
colnames(sidf)<-c('Jabbrev','apc','SI','SI_DOI','ed1','ed1affil','ed1email',
                  'ed2','ed2affil','ed2email',
                  'ed3','ed3affil','ed3email')
thisrow<-0

for (j in 1:10){

jsiname <-paste0("https://www.hindawi.com/journals/",jlist[j],"/psi/")
  mydetails <- read_html(jsiname)
  
  mydetail<-mydetails %>%
   html_elements("body")%>%       
      html_text()
mydetail

aloc<-unlist(gregexpr('APC',mydetail))[1]
apc<-substr(mydetail,(aloc+4),(aloc+7))
x<-unlist(str_split(mydetail,"title"))
x1<-x[75:length(x)] #first 74 entries are boilerplate

for (n in 1:length(x1)){
  titleloc<-4 #the word 'title' has been removed, so title starts at 4th char
  openloc<-unlist(gregexpr('open', x1[n]))
  nameloc<-unlist(gregexpr('name', x1[n]))
  suploc<-unlist(gregexpr('sup', x1[n]))
  emailloc<-unlist(gregexpr('email', x1[n]))
   aliasloc<-unlist(gregexpr('alias', x1[n]))
    articlesloc<-unlist(gregexpr('articles', x1[n]))
  
  thisrow<-thisrow+1
  sidf[thisrow,]<-NA #initialise
  sidf$Jabbrev[thisrow]<-jlist[j]
  sidf$SI[thisrow]<-substr(x1[n], titleloc,(openloc-4))
  sidf$SI_DOI[thisrow]<-substr(x1[n], (aliasloc+9),(articlesloc-4))
  sidf$ed1[thisrow]<-substr(x1[n], (nameloc[1]+7),(emailloc[1]-4))
  sidf$ed2[thisrow]<-substr(x1[n], (nameloc[2]+7),(emailloc[2]-4))
  sidf$ed3[thisrow]<-substr(x1[n], (nameloc[3]+7),(emailloc[3]-4))
  sidf$ed1email[thisrow]<-substr(x1[n], (emailloc[1]+8),(suploc[1]-4))
  sidf$ed2email[thisrow]<-substr(x1[n], (emailloc[2]+8),(suploc[2]-4))
  sidf$ed3email[thisrow]<-substr(x1[n], (emailloc[3]+8),(suploc[3]-4))
  sidf$apc[thisrow]<-apc
  
}

}


```

