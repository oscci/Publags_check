---
title: 'Article processing lag as a red flag for editorial malpractice: a case study
  of Hindawi special issues'
author: "Dorothy Bishop"
date: '2023-01-26'
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
require(here)
require(TSDT) #for unfactor function
require(cowplot)
require(PupillometryR)
require(stringr)
#this from raincloud but mostly not needed
packages <- c("ggplot2", "dplyr", "lavaan", "plyr", "cowplot", "rmarkdown", "readr", "caTools", "bitops")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) { install.packages(setdiff(packages, rownames(installed.packages())))
}

```
## Abstract
    Introduction—what is the topic?
    Statement of purpose?
    Summarize why have other studies not tackled similar research questions?
    How has the research question been tackled?
    How was the research done?
    What is the key impact of the research?

## Introduction  
Scientific publishers need to be constantly alert to the possibility of their journals being hijacked by fraudsters. In the past, the main benefits to fraudsters were career-based: they might be appointed or promoted on the basis of what looked like a strong publication record. In the past decade, fraudulent operations have been discovered that give direct financial benefits to middlemen who charge authors to have papers placed in journals, with the costs varying according to the prestige of the journal. These operations have been called 'papermills', and are growing rapidly enough to give concern to major publishers (COPE & STM, 2022). The published material may be faked using templates and/or AI text generation, or may be genuine work by an author such as student project that would not usually meet threshold for publication in an academic journal. The motivation for authors to use papermills is typically career advancement, which typically depends on achieving a publication in a journal that is indexed in major publication databases such as Thomson ISI's Journal Citation Reports or PubMed. 

There appear to be at least two routes by which papermill products get into reputable journals. The first is when editors are hoodwinked: the faked paper is good enough to be accepted via the usual route, being submitted to an editor who sends it out for peer reviewers, who give a favourable response. The papermill identified by Byrne and Labbé (2017) was of this kind: plausible papers on gene knockouts were generated using a template that could be modified by simply changing the genes and phenotypes under consideration. One variant of this approach is when authorship is sold for a paper once it has been accepted for publication: authorship slots can be purchased, with the cost varying according to the number of authors and the prominence in the authorship list. These new author names are then added prior to publication. Another variant is for authors to recommend peer reviewers who are part of the papermill operation. In large and complex fields, editors may have difficulty identifying peer reviewers and rely on such author recommendations, especially if the recommended reviewers have a track record in the field - which, they may have achieved owingn to papermill publications. 
Much of the emphasis of publishers is in training editors and peer reviewers to be alert to this kind of papermill activity. For instance, Seifert (2021) described how his journal was targeted by papermills, and provided advice on how to recognise fraudulent papers.

The second route, which is the focus of the current paper, is when a complicit editor facilitates papermill publication. There has been little research on this topic, and the motivations of editors who engage in such activity can only be assumed to be financial. As noted by COPE & STM (2022) "A popular route to publication is through a special guest edited issue. Often journals will invite contributions to a special issue on a specific topic and this provides an opening for paper mills to submit often many publications to the same issue. One publisher found a special issue of one of their journals where the guest editor’s identity could not be verified and all of the papers had identified flaws that indicated they were fake." Clearly, where an editor is corrupt, no amount of training is going to solve the problem of papermills. Such cases may be identified by showing a pattern of accepting seriously flawed papers, such as those that use "tortured phrases" (Cabanac & Labbé, 2021), those that include irrelevant citations (used by papermills to increase prominence of their papers), are impossible to understand, or  are not related to the topic of the special issue. Publishers should be able to confirm whether the peer review process has been compromised, but to date have been slow to act when problems are reported to them, perhaps because identification of such issues requires one to read the papers in question and form a subjective judgement of their quality. 

The focus of the current paper is on another potential indicator of editorial malpractice, which is an unusually short lag between receipt of a paper by a journal and the initial response to the author: Editor Response Time

What would be a plausible range of days to get an initial response from an editor? In the optimal scenario, the editor allocates reviewers on the day that the paper is submitted, the reviewers immediately agree to review the paper and produce their reviews in a timely fashion: most editors would be very happy if this occurred within a week. The reviews are returned to the author, who revises the paper accordingly and resubmits it to the journal. In theory, this whole process could be achieved within one week; however, in practice, this seldom occurs. The editor may take some time before allocating reviewers; it is often hard to find reviewers; reviewers may take weeks to produce their reviews; the author may then take weeks to address the reviewer concerns. The typical time course of this process will vary from one discipline to another, and will also depend on the diligence of the editor and the complexity of the paper. In general, fast processing times are to be welcomed; however, when an editor has a pattern of unusually fast response times, this can be a red flag that the usual peer review process has been subverted. 

The goal of this paper is to assess Editor Response Times in a set of Hindawi journals that have been flagged up on the website PubPeer as having numerous problematic papers in special issues. Editor Response Times for papers in special issues will be compared with those for regular articles in the same journal, to test the hypothesis that unfeasibly short lags characterise some special issues.
In addition, it is predicted that special issues with short Editor Response Times will have a higher number of comments on PubPeer than special issues with normal range Editor Response Times. 

## Methods 
### Database. 
Information was extracted for the period from Jan 1 2022 to Jan 20 2023 from the websites for a set of ten journals, selected because they had been identified by **Smut Clyde** and **Parashorea x** as having numerous articles with PubPeer comments:   <ul>
Computational and Mathematical Methods in Medicine
Evidence-Based Complementary and Alternative Medicine
Wireless Communications and Mobile Computing
Computational Intelligence and Neuroscience
Journal of Environmental and Public Health
Mobile Information Systems
Journal of Healthcare Engineering
Journal of Sensors
Security and Communication Networks
Mathematical Problems in Engineering
</ul>  

### Variables
The following variables were extracted from the journal website for each article:  
Publication year, article	ID, doi, title, author list (as a single string), email of first author, date received, date revised, date accepted, date published, editor, name of special issue (or 0 if regular article).  

Additional variables were computed:  
__Retracted__: Where an article was followed by a subsequent articles of the same title prefaced by 'Retraction', both the original article and the Retraction were linked by a code.  

__Editor Response Time__: This was computed from dates of receipt of submission and revision. This is an indirect and minimum index of Editor Response Time (RT), as we cannot know the time it takes and author to write their revision, but Editor RT cannot be greater the interval between 'revision received' and 'submission received'. Thus, insofar as it is inaccurate, this method will overestimate Editor RT, i.e., will give editors the benefit of the doubt, when looking for unfeasibly brief lags.

Most articles undergo revision after peer review; if there is no revision, this implies that the reviews were so positive that no revision was needed, in which case one can use the interval between 'submission received' and date of acceptance as an index of Editor RT.  
In sum, the variable Editor RT was computed in days as:    
Date of Revision - Date Received  
except in cases where there was no Date of Revision, in which case it was:  
Date of Acceptance - Date Received  

__PubPeer comment__: Coded as binary 0 or 1 for each article. This was coded using data provided by Brandon Snell of the PubPeer Foundation.  Presence of a PubPeer comment on an article cannot be taken as an indicator of problems: some comments may be neutral or positive, or raise unwarranted concerns. However, a number of sleuths, including this author, have used PubPeer to flag up indicators of potential fraud, such as plagiarism, "tortured phrases", inappropriate citations, off-topic content, nonsensical formulae and figures, or odd author affiliations; a spot check (described below) showed that virtually all comments on this dataset were of this kind. 

__Email domain of author__: This was coded for each article as academic or non-academic. This judgement was based solely on whether the domain name ended with a country code or .edu: domains were not checked for legitimacy.

### Analysis of special issues
For each journal, plots of distributions of Editor RT were made for special issues with at least 25 articles, and for all Regular articles in the same journal. The Regular articles were used to compute a cutoff for Editor RT, based on the 5th percentile: where a special issue had many articles with Editor RT shorter than this cutoff, this suggests normal peer review may not have occurred . 

### Analysis of editors
Analysis was restricted to editors who had handled at least 30 articles. For each Editor of a special issue, the proportion of Editor RTs below the 5th percentile cutoff was determined. Using this approach, a set of editors was selected for further scrutiny.  

A table was created showing, for each selected Editor, whether the editor had an academic email, whether the editor's academic affiliation/activity could be readily found on Google search,  how many special issues the editor had been involved with, how many special issue papers were handled (across all journals), the median Editor RT, the proportion of articles with non-academic emails, and the proportion of articles with PubPeer comments. 


## Results  

```{r readfile,include=FALSE}
mydf<-read.csv(here('Hindawi_2022_2023_with_pubpeer.csv'))
mydf$origrow<-mydf$row

w<-which(is.na(mydf$ID))
mydf<-mydf[-w,]
df2<-mydf[!duplicated(mydf$ID),] 
mydf$row<-1:nrow(mydf)

#add email domain columns
mydf$edomain<-NA
mydf$ecountry<-NA
mydf$academail<-1
for (r in 1:nrow(mydf)){
mydf$edomain[r]<-unlist(str_split(mydf$affiliation[r],"@"))[2]
mydf$ecountry[r]<-str_sub(mydf$edomain[r],-3,-1)

}
w<-which(mydf$ecountry %in% c('cat','com','eus','krd','net','nfo','org','sia'))
mydf$academail[w]<-0
```

```{r retractions,include=FALSE}
#retractions
mydf$retracted<-NA
matches<-unlist(gregexpr('Retracted:', mydf$title))
w<-which(matches> -1) #row number of those with 'Retracted:'
mydf$retracted[w]<- 0 #flag retraction with 0 (identifies retraction even if not matched)
#Can we match it with an earlier paper? (may not, if it was in previous year)
if(length(w)>0){
  for (r in 1:length(w)){
    
    trunctitle<-substring(mydf$title[w[r]],12,nchar(mydf$title[w[r]])) #title without "Retraction:"
    ww<-which(mydf$title==trunctitle)
    if(length(ww)>0){
      mydf$retracted[ww]<-w[r]
      mydf$retracted[w[r]]<-(-ww)
    }
  }
}

```


```{r publicationlag,include=FALSE}
#NB if re-reading a file that is already processed, this won't work!

#convert to R date format
#NB if it was opened in xls then format may have changed! Need to check
datecol<-which(colnames(mydf)=='received')
daterange<-datecol:(datecol+3)
#date format as varies by journal! 
#need to preprocess


#convert space to hyphen
for (d in daterange){
mydf[,d]<-gsub(" ","-",mydf[,d])  
mydf[,d]<-gsub("-20","-",mydf[,d]) 
mydf[,d]<-gsub("Sept","Sep",mydf[,d]) 
}


  dateformat<-'%d-%b-%y' #all should now be in this format
  for (d in daterange){
  mydf[,d]<-as.Date(mydf[,d],dateformat)
  }

mydf$days_rec_rev<-as.numeric(mydf$revised-mydf$received)
mydf$days_rec_acc<-as.numeric(mydf$accepted-mydf$received)

#For ones with wrong date format, lag will be a large negative number
#For retractions, lag is zero

mydf$daysresp<-mydf$days_rec_acc
w<-which(!is.na(mydf$days_rec_rev))
mydf$daysresp[w]<-mydf$days_rec_rev[w]
w1<-which(mydf$days_rec_acc<0) #cases where accepted date not recorded


```

```{r tabulate_SIs,include=FALSE}
#Need a table of special issues
#Also short journal names (different from the keys)

sjournals<-read.csv('Smut_journals.csv')
mydf$abbrev<-NA
mydf$short<-NA
for (sj in 1:nrow(sjournals)){
  thisj<-sjournals$title[sj]
  w<-which(mydf$Journal==thisj)
  mydf$abbrev[w]<-sjournals$abbrev[sj]
  mydf$short[w]<-sjournals$shortname[sj]
}
mydf$longSI<-paste0(mydf$short,": ",mydf$SI)

SItab<-data.frame(table(mydf$longSI))
SItab<-SItab[SItab$Freq>25,] #only special issues with more than 25 articles
SItab$ID<-1:nrow(SItab)
write.csv(SItab,'SItab.csv',row.names=F)
```

```{r tabulate_editors,include=FALSE}
#First just ensure all editor names are in title case (otherwise get duplicates iwth different cases)
mydf$editor<-str_to_title(mydf$editor)
#remove stops from author names
mydf$editor<-gsub(".","",mydf$editor,fixed=TRUE)   #fixed needed or else . treated as wildcard
```

```{r oldeditortable}

myed<-data.frame(table(mydf$editor))
#Remove those who handled fewer than 5 papers

myed<-myed[myed$Freq>4,]
myed<-myed[order(-myed$Freq),]
colnames(myed)<-c("Editor","Npapers")
myed$Editor<-unfactor(myed$Editor)

#Editors in special issues
myedsi<-data.frame(table(mydf$editor[mydf$SI!=0]))
myedsi<-myedsi[myedsi$Freq>4,]
myedsi<-myedsi[order(-myedsi$Freq),]
colnames(myedsi)<-c("Editor","Npapers")
myedsi$Editor<-unfactor(myedsi$Editor)

myed$Nsi<-0
for (e in 1:nrow(myed)){
  w<-which(myedsi$Editor==myed$Editor[e])
  if(length(w)>0){
    myed$Nsi[e]<-myedsi$Npaper[w]
  }
}

myed$medianlag<-NA
myed$pbelow20<-NA

#remove cases where date formatting gave negative lags, and retractions
w<-which(mydf$daysresp<1)
mydf2<-mydf[-w,]

for (e in 1:nrow(myed)){
  w<-which(mydf2$editor==myed$Editor[e])
  myed$medianlag[e]<-median(mydf2$daysresp[w],na.rm=T)
  myed$pbelow20[e]<-100*length(which(mydf2$daysresp[w]<20))/length(w)
  
}

myed<-myed[order(-myed$pbelow20),]
edfocuslist<-mydf[FALSE,] #create an emplty clone df to hold editors of interest

```


```{r CIs}
#For each journal, compute 95% CI for Editor RT for regular articles.
sjournals<-read.csv('Smut_Journals.csv')

ssjournals<-sjournals[1:10,]

sjournals$medlag<-NA
sjournals$lowcut<-NA


for (j in 1:10){
  jdf<-filter(mydf,Journal==ssjournals$title[j],SI==0)
  sjournals$medlag[j]<-median(jdf$daysresp,na.rm=T)
  sjournals$lowcut[j]<-quantile(jdf$daysresp,.05,na.rm=T)
}

write.csv(sjournals,'Smut_journals.csv',row.names=F)
```

```{r rainclouds,include=FALSE}
#dir.create(here('figs'))
# width and height variables for saved plots
w=6
h=3

#add SI abbreviated names to mydf - we do this only for the 10 journals of interest
# - use hand-crafted file SItab_cut
 mydf$SIabbrev<-NA
  SInames<-read.csv('SItab_cut.csv')
for (ss in 1:nrow(SInames)){
  w<-which(mydf$SI==trimws(SInames$Special.Issue[ss])) #trimws removes leading/trailing white space
  if(length(w)>0){
  mydf$SIabbrev[w]<-SInames$ID[ss]
  }
}

for (j in 1:10){

  myd<-filter(mydf,Journal==ssjournals$title[j])
  thisj<-myd$abbrev[1] #My abbreviation of title for the figure
thisj<-myd$Journal[1]

jsi <-levels(as.factor(myd$SIabbrev))

sitab<-data.frame(table(myd$SIabbrev))
#remove SI with 25 or fewer papers
sitab<-sitab[sitab$Freq>25,]

usesi<-unique(unfactor(sitab$Var1))
myd2<-myd[myd$SIabbrev %in% usesi,]


myd2$days_trunc<-myd2$daysresp
w<-which(myd2$days_trunc>140)
myd2$days_trunc[w]<-140
myd2<-myd2[myd2$days_trunc>0,] #removed negatives and retractions
w<-which(is.na(myd2$SIabbrev))
if(length(w)>0){
  myd2<-myd2[-w,]}


myd2$nued<-myd2$editor #need to find a way of omitting those with few papers
w<-which(myd2$SI==0)
myd2$nued[w]<-'_REGULAR EDITORS'
myd2$SIabbrev[w]<-'_OTHER'
myt<-data.frame(table(myd2$nued))
keeped<-which(myt$Freq>20)
keeps<-myt$Var1[keeped]
myd3<-myd2
myd3<-myd3[myd3$nued %in% keeps,]


#Basic plot
jtitle<-thisj
if(j==2){
  jtitle<- 'Evidence-Based Complementary & Alternative Med'}
#orig name too long
p1<-ggplot(myd2,aes(x=SIabbrev,y=days_trunc, fill = SIabbrev, colour = SIabbrev))+
geom_flat_violin(position = position_nudge(x = .25, y = 0),adjust =2)+
geom_point(position = position_jitter(width = .15), size = .25)+
 ylab('Days')+xlab('')+coord_flip()+theme_cowplot()+guides(fill = "none", colour = "none") +
ggtitle(jtitle)


p2<-ggplot(myd3,aes(x=nued,y=days_trunc, fill = SIshort, colour = SIshort))+
geom_flat_violin(position = position_nudge(x = .25, y = 0),adjust =2)+
geom_point(position = position_jitter(width = .15), size = .25)+
 ylab('Days')+xlab('')+coord_flip()+theme_cowplot()+guides(fill = "none", colour = "none") +
ggtitle(jtitle)

p1+geom_hline(yintercept=sjournals$lowcut[j],linetype='dashed')
savename<- paste0('figs/',thisj,'.eps')

ggsave(savename)

# edadd<-myd3[!duplicated(myd3$nued),]
# edadd<-edadd[edadd$nued!='_REGULAR EDITORS',]
# edfocuslist<-rbind(edfocuslist,edadd)
}
```


```{r editorscrutiny}
#create list with just one row for each of the editors. 
#Ultimately will do this with aggregate so can also record N retractions
#Also of interest to see the email domains of authors

#look at editors from SI papers across journals.

#first filter to just keep the journals of interest and the SI papers only
sjournals<- read.csv('Smut_journals.csv')
jlist<-sjournals$title[1:10]

dfe<-filter(mydf,Journal %in% jlist,SI!=0)

#A table was created showing, for each selected Editor, whether the editor had an academic email, whether the editor's academic affiliation/activity could be readily found on Google search,  how many special issues the editor had been involved with, how many special issue papers were handled (across all journals), the median Editor RT, the proportion of articles with non-academic emails, and the proportion of articles with PubPeer comments. 

etab<-data.frame(table(dfe$editor))
etab<-etab[etab$Freq>29,] #editors who handled at least 30 papers
etab<-etab[-1,] #row 1 is blank, remove it
colnames(etab)<-c('Editor','Npapers')
cutoff<-36 #average lower cutoff for the 10 journals

etab$medianlag<-NA
etab$pbelowcut<-NA
for (e in 1:nrow(etab)){
  w<-which(mydf$editor==etab$Editor[e])
  etab$medianlag[e]<-median(mydf$daysresp[w],na.rm=T)
  etab$pbelowcut[e]<-100*length(which(mydf$daysresp[w]<cutoff))/length(w)
  
}

etab<-etab[order(-etab$pbelowcut),]

write.csv(etab,'etab.csv',row.names=F)

etab$NSI<-NA
etab$PubPeer
etab$retracted
#How many special issues is editor involved in
for (e in 1:nrow(etab)){
  f<-filter(mydf,editor==etab$Editor[e])
  etab$NSI[e]<-length(unique(f$SI))
  etab$PubPeer[e]<-length(unique(f$pubpeer_link))-1
  etab$retracted[e]<-length(which(!is.na(f$retracted)))
}

etab<-etab[order(-etab$Npaper),]

write.csv(etab,'etab.csv',row.names=F)

```

Distributions of Editor RTs in days are shown for special issues and regular articles in raincloud plots (Allen et al, 2021) for each of the 10 journals considered here. A key to Special Issue titles can be found in Supplementary Table 1. Special issues with fewer than 25 articles are omitted from the plots, as well as retractions. The plotted data are censored, so that Editor RTs greater than 200 are shown as 200. 


Table 1 shows further information for all Editors where more than 30% of Editor RTs are below the lower bound of the 95% confidence interval, based on Editor RTs for regular articles. 





## Discussion  

## References  
Allen, M., Poggiali, D., Whitaker, K., Marshall, T. R., Van Langen, J., & Kievit, R. A. (2021). Raincloud plots: A multi-platform tool for robust data visualization [version 2; peer review: 2 approved]. Wellcome Open Research, 15191.2. https://doi.org/10.12688/wellcomeopenres.15191.2  

Byrne, J. A., & Labbé, C. (2017). Striking similarities between publications from China describing single gene knockdown experiments in human cancer cell lines. Scientometrics, 110(3), 1471–1493. https://doi.org/10.1007/s11192-016-2209-6

Cabanac, G., & Labbé, C. (2021). Prevalence of nonsensical algorithmically generated papers in the scientific literature. Journal of the Association for Information Science and Technology, 72(12), 1461–1476. https://doi.org/10.1002/asi.24495  

COPE & STM. (2022). Paper mills: Research report from COPE & STM. Committee on Publication Ethics and STM. https://doi.org/10.24318/jtbG8IHL

Seifert, R. (2021). How Naunyn-Schmiedeberg’s Archives of Pharmacology deals with fraudulent papers from paper mills. Naunyn-Schmiedeberg’s Archives of Pharmacology, 394(3), 431–436. https://doi.org/10.1007/s00210-021-02056-8
  
